{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD2421 Lab 1\n",
    "\n",
    "**Authors:** Navid Farhadi (nfarhadi@kth.se), Arturs Kurzemnieks (artursk@kth.se)\n",
    "\n",
    "**Course Instance:** DD2421 Machine Learning VT19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 0\n",
    "\n",
    "MONK-2 is probably the hardest to learn as the truth concept behind it is the most complex, containing the most clauses. It can be expressed as a disjunction of all possible pairs of attributes, i.e., $(a_1 \\land a_2) \\lor (a_1 \\land a_3) \\lor ... \\lor (a_5 \\land a_6)$\n",
    "\n",
    "A decision tree can represent arbitrary boolean functions therefore all of them can be learned but the truth concept in MONK-2 has a lot more possible combinations of attributes that can make it true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monkdata as m\n",
    "import dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1 Entropy: 1.0\n",
      "MONK-2 Entropy: 0.957117428264771\n",
      "MONK-3 Entropy: 0.9998061328047111\n"
     ]
    }
   ],
   "source": [
    "print(\"MONK-1 Entropy: \" + str(dtree.entropy(m.monk1)))\n",
    "print(\"MONK-2 Entropy: \" + str(dtree.entropy(m.monk2)))\n",
    "print(\"MONK-3 Entropy: \" + str(dtree.entropy(m.monk3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset | Entropy\n",
    " ---| ---\n",
    "MONK-1 | 1.0\n",
    "MONK-2 | 0.957117428264771\n",
    "MONK-3 | 0.9998061328047111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "For a distribution of $n$ events, a uniform distribution will have the most uncertainty (all events are equally likely) and therefore the highest entropy value of $logn$. A non-uniform distribution (of size $n$) will always have lower entropy as it is more biased to some particular event(s), with the minimum entropy value being 0 if it is known that one particular event of the set will always happen. \n",
    "\n",
    "Example: Take a fair six-sided die with $p_1 = \\frac{1}{6}, p_2 = \\frac{1}{6}, ... , p_6 = \\frac{1}{6}$. The distribution is uniform so we get that $ \\text{Entropy} = -\\sum_i p_i \\text{log}_2p_i \\approx 2.58$. If we instead take an unfair six-sided die with $p_1 = \\frac{1}{10}, p_2 = \\frac{1}{10}, ... , p_5 = \\frac{1}{10}, p_6 = \\frac{1}{2}$ we get that $\\text{Entropy} \\approx 2.18$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONK-1: \n",
      "a_1: 0.07527255560831925\n",
      "a_2: 0.005838429962909286\n",
      "a_3: 0.00470756661729721\n",
      "a_4: 0.02631169650768228\n",
      "a_5: 0.28703074971578435\n",
      "a_6: 0.0007578557158638421\n",
      "MONK-2: \n",
      "a_1: 0.0037561773775118823\n",
      "a_2: 0.0024584986660830532\n",
      "a_3: 0.0010561477158920196\n",
      "a_4: 0.015664247292643818\n",
      "a_5: 0.01727717693791797\n",
      "a_6: 0.006247622236881467\n",
      "MONK-3: \n",
      "a_1: 0.007120868396071844\n",
      "a_2: 0.29373617350838865\n",
      "a_3: 0.0008311140445336207\n",
      "a_4: 0.002891817288654397\n",
      "a_5: 0.25591172461972755\n",
      "a_6: 0.007077026074097326\n"
     ]
    }
   ],
   "source": [
    "print(\"MONK-1: \")\n",
    "for i in range(6):\n",
    "    print(\"a_\" + str(i+1) + \": \" + str(dtree.averageGain(m.monk1,m.attributes[i])))\n",
    "    \n",
    "print(\"MONK-2: \")\n",
    "for i in range(6):\n",
    "    print(\"a_\" + str(i+1) + \": \" + str(dtree.averageGain(m.monk2,m.attributes[i])))\n",
    "\n",
    "print(\"MONK-3: \")\n",
    "for i in range(6):\n",
    "    print(\"a_\" + str(i+1) + \": \" + str(dtree.averageGain(m.monk3,m.attributes[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset | $a_1$ | $a_2$ | $a_3$ | $a_4$ | $a_5$ | $a_6$\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "MONK-1 | 0.07527255560831925 | 0.005838429962909286 | 0.00470756661729721 | 0.02631169650768228 | 0.28703074971578435 | 0.0007578557158638421\n",
    "MONK-2 | 0.0037561773775118823 | 0.0024584986660830532 | 0.0010561477158920196 | 0.015664247292643818 | 0.01727717693791797 | 0.006247622236881467\n",
    "MONK-3 | 0.007120868396071844 | 0.29373617350838865 | 0.0008311140445336207 | 0.002891817288654397 | 0.25591172461972755 | 0.007077026074097326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using a single decision tree for all of the MONK data sets it is best to use $a_5$ since that has the highest average information gain across the attributes. However, if each MONK data set has its own decision tree then MONK-1 would use $a_5$, MONK-2 would use $a_5$, and MONK-3 would use $a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "\n",
    "As the information gain is maximized, the entropy of the subsets $S_k$ decreases.\n",
    "\n",
    "For greater information gain, logically the attribute chosen should have the most direct correlation with how the labels are split between the samples. If for some attribute value $k$ the labels are still distributed somewhat evenly, it indicates that the attribute having this particular value has little effect on the label, i.e., the entropy is high. \n",
    "\n",
    "A subset $S_k$ having smaller entropy means that the samples with the attribute value $k$ are more biased to some particular label. If all subsets $S_k$ have small entropy values, it's safe to conclude that the attribute we're looking at is in general strongly affecting the labeling, therefore choosing the attribute with the highest information gain is somewhat choosing the most obvious path, looking first at the attribute that gives the most obvious effect on the labeling.\n",
    "\n",
    "After the split, the new resulting sets (subsets of original $S$) should have smaller entropy as the split was based on some attribute's effect on the labeling and most likely a higher number of samples with the same label ended up in the same subset. If the reduced set entropy is still higher than zero, it implies that there is still something else affecting the labeling. Then we can look at all the remaining attributes and how their values affect the entropy of the particular subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
